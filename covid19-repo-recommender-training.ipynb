{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import string\n",
    "import emoji\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import mlflow\n",
    "from mlflow import pyfunc\n",
    "import requests\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize \n",
    "from nltk.stem import WordNetLemmatizer \n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "pd.set_option('display.max_rows',500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Import Dataset\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_github_data = pd.read_csv('./covid-19-repo-data/data/tsv/2020-04-06.tsv', sep='\\t', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Clean Dataset\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtering down to repos that are likely needing contributors based on past behavior\n",
    "raw_github_data_filtered = raw_github_data[(raw_github_data['has_merged_prs'] == True) &\n",
    "    (raw_github_data['has_readme'] == True) &\n",
    "    (pd.isna(raw_github_data['repo_description']) == False) &\n",
    "    (pd.isna(raw_github_data['primary_language_name']) == False) &\n",
    "    (raw_github_data['count_distinct_contributors'] >=2)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect language with error handling\n",
    "def detect_with_error_handle(x):\n",
    "    try:\n",
    "        return detect(x)\n",
    "    except:\n",
    "        return 'Error'\n",
    "    \n",
    "# Check for only latin characters\n",
    "def has_only_latin_letters(text):\n",
    "    char_set = string.printable + '—'\n",
    "    return all((True if x in char_set else False for x in text))\n",
    "\n",
    "# Remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    punctuation_list = string.punctuation + '—'\n",
    "    return text.translate(str.maketrans('', '', punctuation_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Full set of text processing\n",
    "\n",
    "# check language, limit to english, and limit repo's with latin characters. Emojis are converted in the process\n",
    "raw_github_data_filtered['language'] = raw_github_data_filtered['repo_description'].apply(lambda x: 'None' if pd.isna(x) else detect_with_error_handle(str(x)))\n",
    "raw_github_data_filtered = raw_github_data_filtered[raw_github_data_filtered['language'] == 'en'].copy()\n",
    "raw_github_data_filtered['is_latin_only_characters'] = raw_github_data_filtered['repo_description'].apply(lambda x: has_only_latin_letters(emoji.demojize(x)))\n",
    "raw_github_data_filtered = raw_github_data_filtered[raw_github_data_filtered['is_latin_only_characters'] == True].copy()\n",
    "\n",
    "# clean up repo description, topic, and language, combine into one big bag o' words\n",
    "raw_github_data_filtered['repo_description_cleaned'] = raw_github_data_filtered['repo_description'].apply(lambda x: remove_punctuation(x))\n",
    "raw_github_data_filtered['topics'] = raw_github_data_filtered.apply(lambda x: remove_punctuation(str(x['topics']).replace(',','').replace('nan','')), axis=1)\n",
    "raw_github_data_filtered['topics'].fillna('', inplace=True)\n",
    "raw_github_data_filtered['description_plus_topics'] = raw_github_data_filtered['repo_description_cleaned']+' '+raw_github_data_filtered['topics']+' '+raw_github_data_filtered['primary_language_name']\n",
    "raw_github_data_filtered.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Tokenize\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create class to be used by tokenizer to lemmatize... which change matches words to their roots\n",
    "class LemmaTokenizer(object):\n",
    "    def __init__(self):\n",
    "        self.wnl = WordNetLemmatizer()\n",
    "    def __call__(self, articles):\n",
    "        return [self.wnl.lemmatize(t) for t in word_tokenize(articles)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of stop words that should be removed before tokenizing\n",
    "stopwords = list(ENGLISH_STOP_WORDS) + ['covid19','coronavirus','virus','corona','covid','pandemic','sarscov2','outbreak','19','disease','2019','2019ncov','cord19','repository','repo','2020','20','covid2019','covidvirus', 'cases','case']\n",
    "\n",
    "# Create vectorizor of n-grams using stop words and lemmatizer\n",
    "word_vectorizer = CountVectorizer(ngram_range=(1,1), analyzer='word',stop_words=stopwords, tokenizer=LemmaTokenizer())\n",
    "\n",
    "# Fit vectorizer on existing list of repos and create sparse matrix\n",
    "sparse_vector_matrix = word_vectorizer.fit_transform(raw_github_data_filtered['description_plus_topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Build predict function\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_recommender(input_df, word_vectorizer=word_vectorizer,  sparse_vector_matrix = sparse_vector_matrix, repo_lookup=repo_lookup):\n",
    "    \n",
    "    input_df['bag_of_words'] = input_df.apply(lambda x: ' '.join(x), axis = 1)\n",
    "    \n",
    "    # vectorize the inputted string\n",
    "    #inputted_vector = word_vectorizer.transform(pd.Series(str(input_string)))\n",
    "    inputted_vector = word_vectorizer.transform(input_df['bag_of_words'])\n",
    "    \n",
    "    # calculate cosine similarity with existing matrix\n",
    "    one_dimension_cosine_sim = cosine_similarity(inputted_vector, sparse_vector_matrix)\n",
    "\n",
    "    # creating a Series with the similarity scores in descending order\n",
    "    score_series = pd.Series(one_dimension_cosine_sim[0]).sort_values(ascending = False)\n",
    "    # only show matches that have some similarity\n",
    "    score_series = score_series[score_series>0]\n",
    "\n",
    "    # getting the indexes of the 10 most similar movies\n",
    "    top_10_indexes = list(score_series.iloc[1:11].index)\n",
    "    \n",
    "    # initializing the empty list of recommended repo\n",
    "    recommended_repos = []\n",
    "    # populating the list with the titles of the best 10 matching movies\n",
    "    for i in top_10_indexes:\n",
    "        recommended_repos.append({'repo_url':repo_lookup.loc[i,'github_repo_url'],\n",
    "                                  'repo_description':repo_lookup.loc[i,'repo_description']})\n",
    "        \n",
    "    return [recommended_repos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Log to MLflow\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# connect to MLflow\n",
    "mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "mlflow.set_experiment(\"Covid19RepoRecommender\") # creates an experiment if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_conda_env = {\n",
    " 'name': 'mlflow-env',\n",
    " 'channels': ['defaults',\n",
    "              'conda-forge'],\n",
    " 'dependencies': ['python=3.6.2',\n",
    "                  'nltk=3.4.5',\n",
    "                  'nltk_data',\n",
    "                  {'pip': ['mlflow==1.6.0',\n",
    "                           'scikit-learn',\n",
    "                           'cloudpickle==1.2.2']}\n",
    "                 ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class covid19RepoReco(pyfunc.PythonModel):\n",
    "   \n",
    "    ## defining objects needed for leadsModel prediction. \n",
    "    def __init__(self,\n",
    "                 word_vectorizer,\n",
    "                 sparse_vector_matrix,\n",
    "                 repo_lookup,\n",
    "                 text_recommender):\n",
    "        \n",
    "        ## Setting up all needed objects\n",
    "        self.word_vectorizer = word_vectorizer\n",
    "        self.sparse_vector_matrix = sparse_vector_matrix\n",
    "        self.repo_lookup = repo_lookup\n",
    "        self.text_recommender = text_recommender\n",
    "    \n",
    "    ## define function with processing and feeding data into prediction at the end\n",
    "    def predict(self,context,model_input):\n",
    "        return self.text_recommender(model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name=\"Covid Repo Recommender\") as run:\n",
    "    mlflow.log_param(\"num_repos_returned\", 10)\n",
    "    \n",
    "    pyfunc.log_model(\n",
    "        artifact_path = \"covid_repo_reco_pyfunc\",\n",
    "        python_model = covid19RepoReco(word_vectorizer = word_vectorizer,\n",
    "                                       sparse_vector_matrix = sparse_vector_matrix,\n",
    "                                       repo_lookup = repo_lookup,\n",
    "                                       text_recommender = text_recommender),\n",
    "        conda_env = mlflow_conda_env\n",
    "    )\n",
    "    \n",
    "    run_id = run.info.run_uuid\n",
    "    experiment_id = run.info.experiment_id\n",
    "    \n",
    "    mlflow.end_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "### Deploy to Sagemaker\n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: this requires a MLflow pyfunc docker container to already exist in sagemaker\n",
    "\n",
    "import mlflow.sagemaker as mfs\n",
    "\n",
    "\n",
    "# we pull the run and experiment id's from above to create this mlflow location\n",
    "model_uri = \"mlruns/%s/%s/artifacts/covid_repo_reco_pyfunc\" % (experiment_id,run_id)\n",
    "\n",
    "# The region is chosen, pick whats close to you or your systems!\n",
    "region = \"us-east-1\"\n",
    "# The aws account id can be found in the console\n",
    "aws_account_id = \"XXXXXX\"\n",
    "# We use these inputs to automatically reference the sagemaker docker container\n",
    "image_url = aws_account_id \\\n",
    "            + \".dkr.ecr.\" \\\n",
    "            + region \\\n",
    "            + \".amazonaws.com/mlflow-pyfunc:1.5.0\"\n",
    "\n",
    "# now we specify the role that we setup for sagemaker in the previous step\n",
    "sagemaker_arn = \"arn:aws:iam::XXXXXX:role/AmazonSageMakerFullAccess\"\n",
    "\n",
    "\n",
    "# finally, we pick a name for our endpoint within sagemaker\n",
    "endpoint_name = \"covid19-repo-rec\" \n",
    "\n",
    "\n",
    "# with all of the inputs, we run the following to deploy the model it sagemaker\n",
    "mfs.deploy(app_name=endpoint_name, \n",
    "           model_uri=model_uri,\n",
    "           region_name=region,\n",
    "           mode=\"replace\", #this should change to replace if the endpoint already exists\n",
    "           execution_role_arn=sagemaker_arn,\n",
    "           image_url=image_url, \n",
    "           instance_type='ml.t2.medium')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlflow-vir-env",
   "language": "python",
   "name": "mlflow-vir-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
